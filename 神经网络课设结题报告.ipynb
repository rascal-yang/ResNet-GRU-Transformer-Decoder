{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络与深度学习课设结题报告\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  一、任务说明\n",
    "\n",
    "## 1.1 任务目标\n",
    "![title](1.png)\n",
    " - 本次课设的任务目标是开发一个能够自动识别服饰图像中的关键特征并生成对应的自然语言描述的模型。这要求模型不仅能够理解图像中的视觉内容，如服饰的种类、颜色、款式和材质，还要能够根据这些特征产生准确、流畅且语义丰富的文字描述（自然语言）。\n",
    " \n",
    "## 1.2 任务数据集\n",
    "- 本次课设任务使用的是DeepFashion-MultiModal数据集（yumingj/DeepFashion-MultiModal: A large-scale high-quality human dataset with rich multi-modal annotations (github.com)），包括服饰图像及相应的文本描述。以下是对DeepFashion-MultiModal数据集的说明：\n",
    "- **内容组成**：DeepFashion-MultiModal数据集是一个专门为服饰图像识别和描述而设计的大型数据集。它包含了大量的服饰图像，每张图像都配有相应的文本描述。这些描述涵盖了服饰的多种属性，如样式、颜色、材质等。\n",
    "\n",
    "- **图像特征**：数据集中的图像覆盖了广泛的服饰种类和风格，从日常休闲装到正式场合的服装。图像中的服饰呈现出多样的颜色、图案、款式和材料，为模型提供了丰富的视觉特征学习资源。\n",
    "\n",
    "- **文本描述**：每个图像的文本描述不仅包括基本的服饰信息，还可能包含更细节的描述，如服饰的搭配建议、适用场景和流行趋势。这些描述提供了丰富的语义信息，有助于训练模型理解和生成有关服饰的自然语言描述。\n",
    "\n",
    "- **多模态性**：作为一个多模态数据集，DeepFashion-MultiModal允许研究者同时处理图像和文本信息，探索视觉内容和语言描述之间的关系。这种多模态特性对于训练能够理解复杂场景和生成准确描述的模型至关重要。\n",
    "\n",
    "- 通过在这个数据集上训练，模型能够学习到如何有效地结合图像识别和文本生成，从而在实际应用中提供准确和有用的服饰描述。\n",
    "\n",
    "## 1.3 模型结构\n",
    "- **CNN + GRU**：\n",
    "\n",
    "    这种模型使用卷积神经网络（CNN）来处理图像并提取全局特征。\n",
    "然后，使用门控循环单元（GRU）根据提取的特征生成描述文本。GRU是一种有效的循环神经网络，适合处理序列数据。\n",
    "\n",
    "- **网格/区域表示、ResNet + Transformer解码器**：\n",
    "\n",
    "    这种模型首先将图像分割成多个区域或网格，并对每个区域的特征进行单独提取。\n",
    "首先采用预训炼的Resnet模型对图像进行特征提取，接着使用Transformer编码器处理这些特征，然后通过Transformer解码器生成描述文本。Transformer结构在处理序列转换任务方面非常有效，特别是它的注意力机制能够捕捉长距离依赖关系。\n",
    "\n",
    "## 1.4 评测标准\n",
    "- **METEOR**：METEOR是一个基于单词对齐的评估指标，它考虑了同义词和语法结构的匹配，用于评估生成的描述与参考描述之间的相似度。\n",
    "- **ROUGE-L**：ROUGE-L主要用于评估文本摘要和机器翻译，关注最长公共子序列，从而衡量生成的描述与真实描述的一致性。\n",
    "- **BLEU**:BLEU（Bleu: a Method for Automatic Evaluation of Machine Translation - ACL Anthology）是机器翻译评估中常用的指标，通过计算机器生成的文本与一组参考文本的重叠程度来评分，主要用于评估生成文本的准确性和流畅性。\n",
    "\n",
    "\n",
    "#  二、实验数据\n",
    "\n",
    "![title](2.png)\n",
    "\n",
    "## 2.1 数据集来源\n",
    "- **DeepFashion-MultiModal数据集的数据来源是DeepFashion数据集**。DeepFashion数据集是由香港中文大学的研究人员提出的一个大规模时尚图像数据集，它包含了**超过800,000张时尚图像**，其中包括**超过50个类别的衣服和配件**。这些图像是从互联网上收集而来的，并经过了严格的筛选和处理，以确保图像质量和数据质量。\n",
    "- DeepFashion-MultiModal数据集是从DeepFashion数据集中选取了一部分图像，并对其进行了进一步的注释和处理，以增加数据集的多模态性和应用价值。具体来说，**DeepFashion-MultiModal数据集对DeepFashion数据集中的12,701张全身人类图像进行了手动注释**，包括人体分割标签、关键点、DensePose、衣服形状和纹理的属性注释以及文本描述。这些注释信息使得DeepFashion-MultiModal数据集成为了一个非常有价值的资源，它可以应用于多种任务，例如基于文本的人类图像生成、基于文本的人类图像操作、基于骨架的人类图像生成、人体姿态估计、人类图像字幕生成、人类图像的多模态学习、人类属性识别、人体分割预测等等。\n",
    "\n",
    "\n",
    "## 2.2 数据集类型\n",
    "- DeepFashion-MultiModal数据集的是多模态的，每个图像都具有丰富的注释信息，包括**人体分割标签、关键点、DensePose、衣服形状和纹理的属性注释以及文本描述**。这些注释信息使得该数据集可以应用于多种任务，例如基于文本的人类图像生成、基于文本的人类图像操作、基于骨架的人类图像生成、人体姿态估计、人类图像字幕生成、人类图像的多模态学习、人类属性识别、人体分割预测等等。\n",
    "![title](3.png)\n",
    "\n",
    "## 2.3 数据集规模\n",
    "- DeepFashion-MultiModal数据集包含 **44,096 张高分辨率的人类图像**，包括 **12,701 张全身人类图像**。每张全身图像被手动标注了 24 类人体解析标签，并且被手动标注关键点。每张图像都手动标注了衣服形状和纹理的属性。我们为每张图像提供文本描述。\n",
    "\n",
    "## 2.4 数据集文件结构\n",
    "![title](4.png)\n",
    "\n",
    "- 如上图所示，DeepFashion-MultiModal数据集存放在data/multimodel路径下。其中，images存放着**12,701 张全身人类图像**。train_captions.json和test_captions.json文件是训练集和测试集，其中是对穿着服饰的全身人类图像的文本描述。train.json和valid.json文件是由模型自己划分的。\n",
    "\n",
    "![title](5.png)\n",
    "\n",
    "\n",
    "#  三、实验环境\n",
    "\n",
    "## 3.1 硬件环境\n",
    " \n",
    "- 处理器： Intel(R) UHD Graphics\n",
    "- 显卡：NVIDIA GeForce MX350\n",
    "- 内存：16GB\n",
    "\n",
    "## 3.2 软件环境\n",
    "\n",
    "- 编程语言：Python 3.9.16\n",
    "- 模块：pytorch 1.8.2、torchvision 0.3.0、numpy 1.26.2、pandas 1.2.4、matplotlib 3.5.1、seaborn 0.13.0、tqdm 4.66.1、nltk 3.8.1、gensim 4.3.2\n",
    "\n",
    "## 3.3 开发环境\n",
    "\n",
    "- 操作系统：Windows 10\n",
    "- 代码编辑器：Jupyter Notebook提供了一个方便的界面，用于编写和运行代码的交互式环境，支持.ipynb文件格式，方便展示代码、输出和文档。但也可以辅以其他代码编辑器，如Visual Studio Code，VSCode的Jupyter扩展支持多数Jupyter Notebook的特性，包括内联显示图表、运行进度指示、代码补全等。还允许开发者将Notebook导出为其他格式，如HTML或PDF，以便于展示和分享。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  四、所用方法与模型\n",
    "\n",
    "## 4.1 CNN+GRU\n",
    "\n",
    "### 4.1.1 导入库\n",
    "- 首先导入了torch、torchvision、json、string、os、random、PIL、matplotlib、numpy等模型所需要使用到的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import string\n",
    "from torch.nn.functional import one_hot\n",
    "import os\n",
    "import random \n",
    "from collections import defaultdict, Counter\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from torchvision import transforms\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 分词并构建词汇表和映射\n",
    "1. ***读取 JSON 文件***： 从两个 JSON 文件中读取训练和测试数据。\n",
    "\n",
    "2. ***获取文本描述***： 提取出所有的文本描述。\n",
    "   \n",
    "3. ***分词***： 对描述进行分词，并将所有单词转换为小写。\n",
    "   \n",
    "4. ***构建词汇表***： 统计所有单词的出现次数，并根据出现次数创建词汇表。\n",
    "   \n",
    "5. ***创建词到索引的映射***： 为词汇表中的每个词创建一个索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deepfashion-multimodal/train_captions.json','r') as json_file:\n",
    "    traindata = json.load(json_file)\n",
    "with open('deepfashion-multimodal/test_captions.json','r') as json_file:\n",
    "    testdata = json.load(json_file)\n",
    "\n",
    "# 2. 获取所有文本描述\n",
    "traindescriptions = list(traindata.values())\n",
    "testdescriptions = list(testdata.values())\n",
    "\n",
    "all_words = []\n",
    "test_words= []\n",
    "for description in traindescriptions:\n",
    "    # 先去除标点符号 然后进行分词\n",
    "    words = re.findall(r\"\\b\\w+\\b|[,.']\", description.lower())\n",
    "    all_words.extend(words)\n",
    "\n",
    "for description in testdescriptions:\n",
    "    words = re.findall(r\"\\b\\w+\\b|[,.']\", description.lower())\n",
    "    test_words.extend(words)\n",
    "\n",
    "# 构建词汇表和映射\n",
    "word_counts = Counter(all_words)\n",
    "vocab = ['<pad>', '<unk>', '<eos>','<sta>',\"'\"] + [word for word, _ in word_counts.most_common()]\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 构建键值映射\n",
    "\n",
    "1. **初始化**：创建空的列表用于存储 one-hot 编码和编码长度，同时也初始化一个变量用于记录最大长度。\n",
    "\n",
    "2. **处理每个描述**：对每个训练描述进行处理。首先，分词并添加特殊标记。然后，将每个词转换为 one-hot 编码，并记录编码的长度。如果当前描述的编码长度超过最大长度，就更新最大长度。最后，将每个描述的 one-hot 编码和编码长度分别添加到相应的列表中。\n",
    "\n",
    "3. **构建键值映射**：使用训练数据的键和一开始创建的两个列表（one-hot 编码列表和编码长度列表）构建两个字典。\n",
    "\n",
    "4. **输出结果**：打印词汇表、键-字符索引表长度、键-字符索引表的一个映射、键-字符索引表 size 的一个映射和训练数据中文本数据的最大词数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoded_list = []\n",
    "onehor_encoded_lens = []\n",
    "max_length=0\n",
    "\n",
    "for description in traindescriptions:\n",
    "    words = re.findall(r\"\\b\\w+\\b|[,.']\", description.lower())\n",
    "    words=['<sta>']+words\n",
    "    # if len(words)>98:\n",
    "    #     words = words[0:98]\n",
    "    words.append('<eos>')\n",
    "\n",
    "    onehot_encoded = torch.stack([torch.tensor(word_to_idx.get(word, word_to_idx['<unk>'])) for word in words], dim=0)\n",
    "    onehot_lens = onehot_encoded.size()\n",
    "    if onehot_encoded.size(0)>max_length:\n",
    "        max_length=onehot_encoded.size(0)\n",
    "    onehot_encoded_list.append(onehot_encoded)\n",
    "    onehor_encoded_lens.append(onehot_lens)\n",
    "\n",
    "# 6. 构建key-one-hot编码的映射字典\n",
    "key_onehot_dict = dict(zip(traindata.keys(), onehot_encoded_list))\n",
    "key_onehot_dict_lens = dict(zip(traindata.keys(), onehor_encoded_lens))\n",
    "# 打印词汇表\n",
    "print(\"词汇表:\")\n",
    "print(vocab)\n",
    "print(\"key- 字符索引表长度:\")\n",
    "print(len(key_onehot_dict))  # 10155\n",
    "\n",
    "# 打印key- one-hot编码 的一个映射\n",
    "print(\"key- 字符索引表的一个映射:\")\n",
    "print(next(iter(key_onehot_dict.items())))\n",
    "\n",
    "# 打印\n",
    "print(\"key- 字符索引表size的一个映射:\")\n",
    "print(next(iter(key_onehot_dict_lens.items())))\n",
    "\n",
    "print(\"训练数据中 文本数据最大词数为\")\n",
    "print(max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 加载词嵌入并创建嵌入矩阵\n",
    "\n",
    "1. **加载 GloVe 词嵌入**：定义一个函数 `load_glove_embedding`，该函数从 __GloVe__ 文件中读取预训练的词嵌入。每一行代表一个词的嵌入，其中第一个值是词，后面的值是该词的嵌入向量。这些信息被存储在一个字典中，其中键是词，值是向量。\n",
    "\n",
    "2. **创建嵌入矩阵**：定义一个函数 `create_embedding_matrix`，该函数为给定的词汇表创建一个嵌入矩阵。对于词汇表中的每个词，它检查该词是否在嵌入字典中。如果在，就将矩阵中对应的行设置为该词的嵌入向量。\n",
    "\n",
    "3. **处理特殊标记**：在创建嵌入矩阵的过程中，对于词汇表中的特殊标记（如 '<\\unk>', '<\\pad>', '<\\sta>', '<\\eos>'），会有特殊的处理方式。例如，对于 '<\\unk>'，它的嵌入向量将被设置为一个随机初始化的向量；对于 '<\\pad>'，它的嵌入向量将被设置为一个全零的向量；对于 '<\\sta>' 和 '<\\eos>'，它们的嵌入向量将分别被设置为嵌入字典中 'start' 和 'end' 的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embedding(glove_file):\n",
    "    '''\n",
    "    加载GloVe词嵌入\n",
    "    键是词 值是向量\n",
    "    '''\n",
    "    embedding_dict = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_dict[word] = vector\n",
    "    return embedding_dict\n",
    "\n",
    "def create_embedding_matrix(vocab, embedding_dict, embedding_dim):\n",
    "    '''\n",
    "    为给定的词汇表创建一个嵌入矩阵\n",
    "    对词汇表中的每个词 它检查该词是否在嵌入字典中 如果在 就将矩阵中对应的行设置为该词的嵌入向量\n",
    "    '''\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[i] = embedding_dict[word]\n",
    "        else:\n",
    "            # 处理特殊标记 可以选择使用GloVe中的对应向量或随机初始化的向量\n",
    "            if word == '<unk>':\n",
    "                embedding_matrix[i] = np.random.rand(embedding_dim)\n",
    "            elif word == '<pad>':\n",
    "                embedding_matrix[i] = np.zeros(embedding_dim)\n",
    "            elif word == '<sta>' :\n",
    "                embedding_matrix[i] = embedding_dict['start']\n",
    "            elif word == '<eos>' :\n",
    "                embedding_matrix[i] = embedding_dict['end']\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 GloVe 词典文件\n",
    "1. 定义 GloVe 文件的路径 (`glove_file`) 和嵌入维度 (`EMBED_DIM`)。\n",
    "\n",
    "2. 检查预先分割的 GloVe 词典文件 (`glove.6B/glove_vocab_110.pth`) 是否存在。\n",
    "\n",
    "3. 如果 GloVe 词典文件存在，则加载该文件到 `embedding_matrix_tensor`。\n",
    "\n",
    "4. 如果 GloVe 词典文件不存在，则开始创建新的词典文件."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'glove.6B/glove.6B.300d.txt'  # GloVe文件路径\n",
    "EMBED_DIM= 300  # 你选择的嵌入维度\n",
    "if os.path.exists(\"glove.6B/glove_vocab_110.pth\"):\n",
    "    print(\"glove 词典已划分!\")\n",
    "    embedding_matrix_tensor = torch.load(\"glove.6B/glove_vocab_110.pth\")\n",
    "else:\n",
    "    print(\"glove 词典未划分 正在划分...\")\n",
    "    glove_vocab = \"glove.6B/glove_vocab_110.pth\"\n",
    "    # 从GloVe文件中加载嵌入向量\n",
    "    glove_embeddings = load_glove_embedding(glove_file)\n",
    "    # 创建词汇表-embedding矩阵字典\n",
    "    embedding_matrix = create_embedding_matrix(vocab, glove_embeddings, EMBED_DIM)\n",
    "    # 将 embedding_matrix 转换为 PyTorch 张量\n",
    "    embedding_matrix_tensor = torch.tensor(embedding_matrix)        # (110, 300)\n",
    "    torch.save(embedding_matrix_tensor, glove_vocab)\n",
    "    print(\"embedding matrix\",embedding_matrix)\n",
    "    print(\"embedding matrix shape\",embedding_matrix.shape)\n",
    "\n",
    "# embedding_matrix_tensor = embedding_matrix_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 自定义数据集类CustomDataset\n",
    "\n",
    "这是一个 PyTorch 自定义数据集类，用于处理图像和文本数据。\n",
    "\n",
    "1. `__init__` 方法：初始化数据集。输入包括：\n",
    "   - `key_onehot_dict`：一个字典，其中的键是图像的文件名，值是对应的 one-hot 编码的文本。\n",
    "   - `key_onehot_dict_lens`：一个字典，其中的键是图像的文件名，值是对应的文本长度。\n",
    "   - `max_length`：文本的最大长度，用于后续的填充操作。\n",
    "   - `transform`：图像的预处理流程，这通常包括归一化、裁剪等操作。\n",
    "\n",
    "2. `__len__` 方法：返回数据集的大小，也就是图像的数量。\n",
    "\n",
    "3. `__getitem__` 方法：获取指定索引的数据。输入是数据的索引 `idx`，输出是对应的图像、文本、文本长度和嵌入向量。这个方法的具体步骤如下：\n",
    "   - 通过索引获取图像的文件名（`key`）。\n",
    "   - 打开并读取对应的图像，将其转换为 RGB 格式。\n",
    "   - 如果定义了图像的预处理流程，那么就对图像进行这个预处理。\n",
    "   - 获取对应的 one-hot 编码的文本和文本长度。\n",
    "   - 计算需要填充的长度，然后创建一个填充向量。如果需要填充的长度大于 0，那么就使用 `<pad>` 对应的索引进行填充，否则就使用一个空的张量。\n",
    "   - 将原始的 one-hot 编码的文本和填充向量拼接起来，得到最终的文本。\n",
    "   - 使用预训练的嵌入矩阵将文本转换为嵌入向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, key_onehot_dict, key_onehot_dict_lens, max_length, transform=None):\n",
    "        self.key_onehot_dict = key_onehot_dict\n",
    "        self.key_onehot_dict_lens = key_onehot_dict_lens\n",
    "        self.max_length = max_length\n",
    "        self.keys = list(key_onehot_dict.keys())\n",
    "        # PyTorch图像预处理流程\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        image = Image.open(\"deepfashion-multimodal/images/\"+key).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        onehot_encoded = self.key_onehot_dict[key]\n",
    "        onehot_len = self.key_onehot_dict_lens[key]\n",
    "\n",
    "        # 填充到最大长度\n",
    "        pad_length = self.max_length - onehot_len[0]\n",
    "\n",
    "        onehot_encoded_max = torch.stack([torch.tensor(word_to_idx['<pad>']) for i in range(pad_length)]) if pad_length > 0 else torch.tensor([])\n",
    "\n",
    "        caption = torch.cat([onehot_encoded, onehot_encoded_max])\n",
    "        embeddings = F.embedding(caption, embedding_matrix_tensor)\n",
    "\n",
    "        return image, caption, onehot_len, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.7 图像编码器与解码器\n",
    "\n",
    "这段代码定义了一个图像标题生成模型，该模型包括一个图像编码器（`EncoderCNN`）和一个解码器（`DecoderGRU`）。\n",
    "\n",
    "1. `EncoderCNN` 类：这是一个图像编码器，它使用预训练的 ResNet-18 模型进行特征提取。最后的全连接层被移除，只保留卷积层。其 `forward` 方法接收图像作为输入，然后通过 ResNet 模型得到特征。\n",
    "\n",
    "2. `DecoderGRU` 类：这是一个 GRU 解码器，它接收图像特征和嵌入向量作为输入，然后生成文本。其主要部分包括：\n",
    "   - `init_hidden_state` 方法：初始化 GRU 的隐藏状态。输入是图像特征，输出是处理后的特征和隐藏状态。\n",
    "   - `forward` 方法：接收图像特征和嵌入向量作为输入，然后通过多次迭代生成预测的文本。在每次迭代中，它使用 GRU 更新隐藏状态，并使用全连接层生成预测的单词。最后，所有预测的单词被连接起来，形成最终的文本。\n",
    "   - `generate_by_beamsearch` 方法：使用束搜索生成文本。这是一种启发式搜索策略，用于在所有可能的输出序列中找到最可能的序列。与 `forward` 方法不同，这个方法在每次迭代中都会生成一个单词，然后将这个单词添加到已生成的文本中。如果生成了结束符，那么就停止生成。\n",
    "\n",
    "3. `ImageCaptioningModel` 类：这是一个图像标题生成模型，它包括一个图像编码器和一个解码器。其 `forward` 方法接收图像和嵌入向量作为输入，然后通过编码器得到图像特征，再通过解码器生成文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]  # Remove the last fully connected layer\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        # self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)          #  torch.Size([32, 512, 7, 7])\n",
    "        # features = features.view(features.size(0), -1)\n",
    "        # features = self.fc(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, max_length, num_layers=1):\n",
    "        super(DecoderGRU, self).__init__()\n",
    "        self.image_code_dim = hidden_size\n",
    "        # self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size + self.image_code_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.init_state = nn.Linear(512, num_layers*hidden_size)\n",
    "        self.fc2 = nn.Linear(512 * 49, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, features, embeddings, length):\n",
    "        '''\n",
    "        参数\n",
    "            features torch.Size([32, 512, 7, 7])\n",
    "            embeddings torch.Size([32, 120, 300])\n",
    "            captions torch.Size([32, 120])\n",
    "\n",
    "        '''\n",
    "\n",
    "        features, hidden_state = self.init_hidden_state(features)\n",
    "        # features torch.Size([32, 49, 512]) \n",
    "        # hidden_state torch.Size([1, 32, 256])\n",
    "        features = self.flatten(features)           # torch.Size([32, 25088])\n",
    "        features = self.fc2(features)               # torch.Size([32, 512])\n",
    "\n",
    "        # 拼接图像特征和文本嵌入\n",
    "        features = features.unsqueeze(1)            # torch.Size([32, 1, 256])\n",
    "        features = features.expand(-1, MAX_LENGTHS, -1)     # torch.Size([32, 120, 256])\n",
    "\n",
    "        inputs = torch.cat((embeddings, features), dim=-1)      # torch.Size([32, 120, 556])\n",
    "\n",
    "        # for i in range(embeddings.shape[1]):        # 120\n",
    "        for i in range(length):\n",
    "\n",
    "            output, hidden_state = self.gru(inputs[:,i:i+1,:], hidden_state)\n",
    "            # output torch.Size([32, 1, 256])\n",
    "            # hidden_state torch.Size([1, 32, 256])\n",
    "        \n",
    "            # 使用线性层映射输出维度\n",
    "            output = self.fc(self.dropout(output))         # torch.Size([32, 1, 110])\n",
    "            \n",
    "            if i > 0:\n",
    "                predicted = torch.cat((predicted, output), dim=1)         # torch.Size([32, n, 110])\n",
    "            else:\n",
    "                predicted = output\n",
    "\n",
    "        # predicted.shape -> torch.Size([32, 120, 110])\n",
    "\n",
    "        return predicted\n",
    "    \n",
    "    def init_hidden_state(self, features):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            features 图像编码器输出的图像表示 \n",
    "                        (batch_size, image_code_dim, grid_height, grid_width)\n",
    "        \"\"\"\n",
    "        # 将图像网格表示转换为序列表示形式 \n",
    "        batch_size, image_code_dim = features.size(0), features.size(1)     # 32 512\n",
    "\n",
    "        # -> (batch_size, grid_height, grid_width, image_code_dim) \n",
    "        features = features.permute(0, 2, 3, 1)  \n",
    "        # -> (batch_size, grid_height * grid_width, image_code_dim)\n",
    "        features = features.view(batch_size, -1, image_code_dim)\n",
    "\n",
    "        # features.mean(axis=1).shape  -> torch.Size([32, 512])\n",
    "\n",
    "        #（2）初始化隐状态\n",
    "        hidden_state = self.init_state(features.mean(axis=1))       # hidden_state size torch.Size([32, 512])\n",
    "\n",
    "        hidden_state = hidden_state.view(\n",
    "                            batch_size, \n",
    "                            self.gru.num_layers, \n",
    "                            self.gru.hidden_size).permute(1, 0, 2)  # hidden_state size torch.Size([1, 32, 512])\n",
    "\n",
    "        return features, hidden_state\n",
    "   \n",
    "    def generate_(self, features, captions):\n",
    "        ''' \n",
    "        测试\n",
    "            embeddings torch.Size([300])\n",
    "            features torch.Size([1, 512, 7, 7])\n",
    "            captions torch.Size([1]) 一般是开始符\n",
    "        '''\n",
    "        embeddings = F.embedding(captions, embedding_matrix_tensor.to(device))\n",
    "        embeddings = embeddings.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        features, hidden_state = self.init_hidden_state(features)\n",
    "        # features torch.Size([1, 49, 512]) \n",
    "        # hidden_state torch.Size([1, 1, 512])\n",
    "        features = self.flatten(features)           # torch.Size([1, 25088])\n",
    "        features = self.fc2(features)               # torch.Size([1, 512])\n",
    "\n",
    "        # 拼接图像特征和文本嵌入\n",
    "        features = features.unsqueeze(1)            # torch.Size([1, 1, 512])\n",
    "\n",
    "        captions = captions.unsqueeze(0)         # tensor([3])\n",
    "\n",
    "        for _ in range(MAX_LENGTHS):\n",
    "            # print(f'epoch\\t{_+1}')\n",
    "            inputs = torch.cat((embeddings, features), dim=-1)      # torch.Size([1, 1, 812])\n",
    "            \n",
    "            output, hidden_state = self.gru(inputs.float(), hidden_state.float())\n",
    "            # hidden_state torch.Size([1, 1, 512])\n",
    "            # output torch.Size([1, 1, 512])\n",
    "\n",
    "            output = self.fc(output)\n",
    "            # torch.Size([1, 1, 110])\n",
    "\n",
    "            _, predicted = output[0][0].max(0)\n",
    "            predicted = predicted.unsqueeze(0)          # torch.Size([1])\n",
    "\n",
    "            captions = torch.cat((captions, predicted), dim=0) \n",
    "            # Append the predicted word to the captions\n",
    "\n",
    "            embeddings = F.embedding(predicted[0], embedding_matrix_tensor.to(device))\n",
    "            embeddings = embeddings.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            # Stop if the end token is generated\n",
    "            if predicted.item() == vocab.index('<eos>'):\n",
    "                break\n",
    "\n",
    "        return captions\n",
    "    \n",
    "    def generate_by_beamsearch(self, features, captions, beam_k=3):\n",
    "        '''\n",
    "        束搜索\n",
    "\n",
    "        在每个时间步\n",
    "        我们首先清空all_candidates列表 然后遍历candidates中的每个序列\n",
    "        对每个序列尝试添加一个新的词 生成新的序列 并将新的序列添加到all_candidates中\n",
    "        然后 我们会对all_candidates中的所有序列进行评分 并选择得分最高的beam_k个序列\n",
    "        将这些序列赋值给candidates 作为下一个时间步的候选序列\n",
    "        这个过程会一直持续到生成的序列达到预定的长度 或者所有的候选序列都已经结束\n",
    "        '''\n",
    "        embeddings = F.embedding(captions, embedding_matrix_tensor.to(device))\n",
    "        embeddings = embeddings.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        features, hidden_state = self.init_hidden_state(features)\n",
    "        # features torch.Size([1, 49, 512]) \n",
    "        # hidden_state torch.Size([1, 1, 512])\n",
    "        features = self.flatten(features)           # torch.Size([1, 25088])\n",
    "        features = self.fc2(features)               # torch.Size([1, 512])\n",
    "\n",
    "        # 拼接图像特征和文本嵌入\n",
    "        features = features.unsqueeze(1)            # torch.Size([1, 1, 512])\n",
    "\n",
    "        # Initialize the list of candidate sequences\n",
    "        candidates = [(captions.unsqueeze(0), 0, hidden_state)]\n",
    "        \n",
    "        for _ in range(MAX_LENGTHS):\n",
    "            # Store all the candidates for the next step\n",
    "            all_candidates = []\n",
    "\n",
    "            for i in range(len(candidates)):\n",
    "\n",
    "                seq, score, hidden_state = candidates[i]\n",
    "                \n",
    "                if seq[-1] == vocab.index('<eos>') or seq[-1] == vocab.index('<pad>'):\n",
    "                    all_candidates.append((seq, score, hidden_state))\n",
    "                    continue\n",
    "                else:\n",
    "                    embeddings = F.embedding(seq[-1], embedding_matrix_tensor.to(device))\n",
    "                    embeddings = embeddings.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "                    inputs = torch.cat((embeddings, features), dim=-1)      # torch.Size([1, 1, 812])\n",
    "\n",
    "                    output, hidden_state_new = self.gru(inputs.float(), hidden_state.float())\n",
    "                    # hidden_state torch.Size([1, 1, 512])\n",
    "                    # output torch.Size([1, 1, 512])\n",
    "\n",
    "                    output = self.fc(output)\n",
    "                    # torch.Size([1, 1, 110])\n",
    "\n",
    "                    output = nn.functional.softmax(output, dim=2)       # torch.Size([1, 1, 110])\n",
    "\n",
    "                    # Get the top k predictions\n",
    "                    topk_probs, topk_ids = torch.topk(output[0, 0], beam_k)\n",
    "                    # torch.Size([5]), torch.Size([5])\n",
    "\n",
    "                    for k in range(beam_k):\n",
    "                        wordID = topk_ids[k]\n",
    "                        prob = topk_probs[k]\n",
    "\n",
    "                        # Add the new word to the sequence and update the score\n",
    "                        seq_new = seq.clone().detach()\n",
    "                        seq_new = torch.cat([seq_new, wordID.unsqueeze(0)])\n",
    "                        score_new = score - torch.log(prob)\n",
    "\n",
    "                        all_candidates.append((seq_new, score_new, hidden_state_new))\n",
    "                \n",
    "\n",
    "            # Sort all candidates by score\n",
    "            ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "\n",
    "            # Select the top k candidates\n",
    "            candidates = ordered[:beam_k]\n",
    "\n",
    "        # Return the sequence with the highest score\n",
    "        return candidates[0][0]\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size,max_length):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderGRU(embed_size, hidden_size, vocab_size,max_length=max_length)\n",
    "    def forward(self, images, embeddings, length):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, embeddings, length)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.8 实例化模型\n",
    "\n",
    "1. **实例化模型**：创建了一个 `ImageCaptioningModel` 的实例，模型的参数包括嵌入层大小（`embed_size`）、隐藏层大小（`hidden_size`）、词汇表大小（`vocab_size`）和最大序列长度（`MAX_LENGTHS`）。`to(device)` 用于将模型放到 GPU 或 CPU 上。\n",
    "\n",
    "2. **定义损失函数和优化器**：使用交叉熵损失函数（`nn.CrossEntropyLoss`）作为损失函数，这是多分类问题的常见选择。优化器选择了 Adam（`torch.optim.Adam`），这是一种常用的自适应学习率优化算法。\n",
    "\n",
    "3. **创建自定义数据集**：实例化了 `CustomDataset` 类，这个类用于处理和加载自定义的数据。传入的参数包括键值对字典、字典长度、最大长度和转换函数。\n",
    "\n",
    "4. **创建数据加载器**：利用 `DataLoader` 类创建了一个数据加载器，这个加载器能够实现数据的批量加载、打乱和并行化等操作。在这里，每个批次的数据大小为 `BATCH_SIZE`，并且每个训练周期开始时，数据会被打乱（`shuffle=True`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioningModel(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size,max_length=MAX_LENGTHS).to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# 创建数据集实例\n",
    "custom_dataset = CustomDataset(key_onehot_dict, key_onehot_dict_lens, max_length=MAX_LENGTHS, transform=transform2ResNet18)\n",
    "# 创建 DataLoader 实例\n",
    "data_loader = DataLoader(dataset=custom_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.9 模型评估\n",
    "\n",
    "1. **模型评估模式**：通过 `model.eval()` 将模型切换到评估模式。在评估模式下，模型的参数不会发生变化，且某些特定的层（如 Dropout 和 BatchNorm）会改变其行为。\n",
    "\n",
    "2. **加载和预处理图像**：随机选择一个图像路径，打开并转换图像为 RGB 格式。然后，应用与训练阶段相同的转换（`transform2ResNet18`），增加一个批次维度，然后将图像移动到设备（GPU 或 CPU）上。\n",
    "\n",
    "3. **图像编码**：使用模型的编码器部分对图像进行编码，得到图像的特征表示。\n",
    "\n",
    "4. **生成标题**：首先获取开始标记（`<sta>`）的索引，并将其转换为一个张量，增加批次和序列维度，然后移动到设备上。然后，使用模型的解码器部分根据图像特征和开始标记生成标题。\n",
    "\n",
    "5. **将标题转换为单词**：将生成的标题从 GPU 移动到 CPU，然后将每个词的索引转换为对应的单词，最后将所有单词连接起来，形成一个完整的句子。\n",
    "\n",
    "6. **显示图像和标题**：使用 matplotlib 显示原始图像。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image_path = random.choice(list(data.keys()))\n",
    "\n",
    "    ima = Image.open(\"deepfashion-multimodal/images/\" + image_path)\n",
    "    image = ima.convert(\"RGB\")\n",
    "    image = transform2ResNet18(image).unsqueeze(0).to(device)  # Apply the same transformation as training, add a batch dimension, and move to device\n",
    "\n",
    "    # Encode the image\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(image)\n",
    "\n",
    "    # Start generating the caption\n",
    "    start_token = vocab.index('<sta>')  # Get the index of the start token\n",
    "    captions = torch.tensor([start_token], dtype=torch.long).unsqueeze(0).to(device)  # Add batch and sequence dimensions and move to device\n",
    "    captions = captions[0]          # torch.Size([1])\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "    # features torch.Size([1, 512, 7, 7])\n",
    "        captions = model.decoder.generate_(features, captions[-1])\n",
    "\n",
    "\n",
    "    # Convert the generated captions to words\n",
    "    captions = captions.cpu().numpy()  # Remove the batch dimension and move to CPU\n",
    "\n",
    "    caption = ' '.join([vocab[i] for i in captions[1:]])\n",
    "    print(caption)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(ima)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.10 模型训练\n",
    "\n",
    "这段代码描述了模型的训练过程，主要步骤如下：\n",
    "\n",
    "1. **开始训练**：在每个训练周期（epoch）开始时，通过 `model.train()` 将模型切换到训练模式。在训练模式下，模型的参数可以更新，某些特定的层（如 Dropout 和 BatchNorm）会改变其行为。\n",
    "\n",
    "2. **加载批次数据**：在每个训练周期中，使用 `tqdm` 进行进度条显示，遍历 `data_loader` 中的每个批次数据。每个批次的数据包括图像、标题、标题长度和嵌入向量。\n",
    "\n",
    "3. **数据预处理**：将图像、标题和嵌入向量移动到设备（GPU 或 CPU）上，将标题转换为长整型。\n",
    "\n",
    "4. **梯度清零**：在每次反向传播之前，首先使用 `optimizer.zero_grad()` 清零梯度。\n",
    "\n",
    "5. **前向传播**：将图像、嵌入向量和标题长度传入模型，进行前向传播，得到输出。\n",
    "\n",
    "6. **计算损失**：调整输出的维度，然后使用损失函数 `criterion` 计算损失。注意，这里忽略了填充部分的损失。\n",
    "\n",
    "7. **反向传播和优化**：使用 `loss.backward()` 进行反向传播，计算梯度，然后使用 `optimizer.step()` 更新模型的参数。\n",
    "\n",
    "8. **计算总损失**：将每个批次的损失累加，得到总损失。\n",
    "\n",
    "9. **计算平均损失**：在每个训练周期结束时，计算平均损失，即总损失除以批次数量。然后打印平均损失，并将其添加到 `losses` 列表中。\n",
    "\n",
    "10. **评估模型**：在每个训练周期结束后，使用 `evaluate` 函数在训练数据上评估模型的性能。\n",
    "\n",
    "11. **返回训练模式**：评估完成后，再次将模型切换到训练模式，准备进行下一个训练周期。\n",
    "\n",
    "这个循环会持续 `EPOCHS` 次，即进行 `EPOCHS` 个训练周期。在每个训练周期中，模型的参数会根据数据的损失进行更新，以达到提升模型性能的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}'):\n",
    "        images = batch[0]\n",
    "        captions = batch[1]\n",
    "        lengths = batch[2]\n",
    "        length = torch.max(lengths[0])\n",
    "        embeddings = batch[3]\n",
    "\n",
    "        images=images.to(device)\n",
    "        captions = captions.long()\n",
    "\n",
    "        captions=captions.to(device)\n",
    "        embeddings = embeddings.to(device).long()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(images,embeddings, length).type(torch.FloatTensor)     # torch.Size([32, 120, 110])\n",
    "\n",
    "        outputs = outputs.permute(0, 2, 1)\n",
    "        outputs = outputs.to(device)            # torch.Size([32, 110, 120])\n",
    "\n",
    "        # Calculate loss (ignoring padding)\n",
    "        loss = criterion(outputs[:,:,:-1], captions[:,1:length])\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{EPOCHS}], Loss: {average_loss:.4f}')\n",
    "    losses.append(average_loss)\n",
    "\n",
    "    evaluate(traindata)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.11 计算ROUGE-L分数\n",
    "\n",
    "这段代码定义了一个函数，用于计算生成描述和参考描述之间的 ROUGE-L 分数。ROUGE-L 是一种基于最长公共子序列（LCS）的评估指标，常用于自然语言处理中的文本生成任务，如摘要生成和机器翻译。\n",
    "\n",
    "1. **导入 Rouge 类**：从 `rouge` 模块中导入 `Rouge` 类。`Rouge` 类是一个用于计算 ROUGE 分数的类。\n",
    "\n",
    "2. **创建 Rouge 对象**：创建一个 `Rouge` 对象 `rouge_scorer`。\n",
    "\n",
    "3. **计算 ROUGE 分数**：使用 `rouge_scorer.get_scores` 方法计算生成描述和参考描述之间的 ROUGE 分数。这个方法返回一个字典，包含了各种 ROUGE 分数。\n",
    "\n",
    "4. **获取 ROUGE-L 分数**：从返回的字典中获取 ROUGE-L 分数。因为 `get_scores` 方法返回的是一个字典，可以使用键 `'rouge-l'` 获取 ROUGE-L 分数，然后使用键 `'f'` 获取 F1 分数。F1 分数是精确度和召回率的调和平均值，是评估模型性能的常用指标。\n",
    "\n",
    "5. **打印 ROUGE-L 分数**：打印计算得到的 ROUGE-L 分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_l_score(pred, reference):\n",
    "    from rouge import Rouge\n",
    "    rouge_scorer = Rouge()\n",
    "    scores = rouge_scorer.get_scores(pred[1:-1], reference, avg=True)\n",
    "\n",
    "    rouge_l_score = scores['rouge-l']['f']\n",
    "    print(rouge_l_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.12 计算METEOR分数\n",
    "\n",
    "这段代码定义了一个函数，用于计算生成描述和参考描述之间的 METEOR 分数。METEOR 是一种用于评估机器翻译的指标，它考虑了精确度、召回率、同义词、词干和词序等因素。\n",
    "\n",
    "以下是这个函数的详细步骤：\n",
    "\n",
    "1. **导入 meteor_score 函数**：从 `nltk.translate.meteor_score` 模块中导入 `meteor_score` 函数。`meteor_score` 函数是一个用于计算 METEOR 分数的函数。\n",
    "\n",
    "2. **准备输入**：将参考标题分割成单词列表，并包装在一个列表中，因为 `meteor_score` 函数需要一个参考描述列表。将生成描述分割成单词列表，移除开始和结束标记。\n",
    "\n",
    "3. **计算 METEOR 分数**：使用 `meteor_score` 函数计算生成描述和参考描述之间的 METEOR 分数。\n",
    "\n",
    "4. **返回 METEOR 分数**：返回计算得到的 METEOR 分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meteor 计算\n",
    "def compute_meteor_score(pred, reference):\n",
    "    from nltk.translate.meteor_score import meteor_score\n",
    "    reference = [reference.split()]\n",
    "    meteor_score_value = meteor_score(reference, pred.split()[1:-1])\n",
    "\n",
    "    return meteor_score_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 网格/区域表示、ResNet+Transformer解码器\n",
    "\n",
    "模型先采用预训炼的Resnet模型对图像进行特征提取，再采用transformer模型对特征进行编码解码，得到图像的文本描述序列\n",
    "- 代码环境\n",
    "  - python 3.9.16 \n",
    "  - pytorch 2.1.0\n",
    "  - torchvision 0.3.0\n",
    "- 导入库\n",
    "  - sys\n",
    "  - os\n",
    "  - json\n",
    "  - torch\n",
    "  - torchvision\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - matplotlib\n",
    "  - seaborn\n",
    "  - tqdm\n",
    "  - nltk\n",
    "  - gensim\n",
    " \n",
    "- 文件结构\n",
    "  - ipynb文件\n",
    "  - data\n",
    "    - deepfashion-multimodal\n",
    "      - images\n",
    "        - 图片\n",
    "      - test_captions.json\n",
    "      - train_captions.json\n",
    "      - train.json     (模型自己划分)\n",
    "      - valid.json     (模型自己划分)\n",
    "    - glove.6B.300d.txt   (预训练词向量，联网下载)\n",
    "\n",
    "### 4.2.1 导入库\n",
    "\n",
    "首先导入了torch、torchvision、json、string、os、random、PIL、matplotlib、numpy、nltk、tqdm等模型所需要使用到的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import string\n",
    "from torch.nn.functional import one_hot\n",
    "import os\n",
    "import random \n",
    "from collections import defaultdict, Counter\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from torchvision import transforms\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "print(torch.__version__)\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 划分训练集和验证集\n",
    "- 若存在则提取并跳过\n",
    "- 若不存在则创建并划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/deepfashion-multimodal/train_captions.json','r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "if os.path.exists(\"data/deepfashion-multimodal/train.json\") and os.path.exists(\"data/deepfashion-multimodal/valid.json\"):\n",
    "    print(\"训练数据集已划分.\")\n",
    "else:\n",
    "    # 从原始数据中随机选择1/5的数据作为验证集\n",
    "    val_size = len(data) // 5\n",
    "    val_data = dict(random.sample(data.items(), val_size))\n",
    "    # 剩余的数据作为训练集\n",
    "    train_data = {k: v for k, v in data.items() if k not in val_data}\n",
    "    with open('data/deepfashion-multimodal/train.json', 'w') as train_file:\n",
    "        json.dump(train_data, train_file, indent=2)\n",
    "    # 将数据写入val.json和train.json\n",
    "    with open('data/deepfashion-multimodal/valid.json', 'w') as val_file:\n",
    "        json.dump(val_data, val_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 对训练集和测试集的样本进行处理\n",
    "- 核心：将文本转换成计算机能理解的矩阵\n",
    "- 步骤：\n",
    "    1. 读取数据集\n",
    "    2. 构建词汇表\n",
    "    3. 将文本转换成数字\n",
    "    4. 将数据集分为训练集和测试集\n",
    "    5. 保存处理后的数据集\n",
    "- **读取测试数据**：\n",
    "   - 使用 `json.load` 从文件中读取测试数据文件 `'data/deepfashion-multimodal/test_captions.json'`，该文件似乎包含图像描述的相关信息。\n",
    "\n",
    "- **处理描述信息**：\n",
    "   - 将测试数据的描述信息提取为列表 `test_descriptions`，其中每个元素是一个图像的描述。\n",
    "\n",
    "- **分词处理**：\n",
    "   - 对每个描述进行处理，首先使用正则表达式 `re.findall(r'\\b\\w+\\b|[,.‘]', description.lower().translate(TRANSALTOR))` 去除标点符号，然后转换为小写，并分词。分词后的结果存储在 `test_all_words` 列表中。\n",
    "\n",
    "- **构建编码列表**：\n",
    "   - 对每个描述进行编码处理。在这里，描述的每个单词（包括 `<sta>` 和 `<eos>`）通过 `word_to_idx` 字典转换为对应的索引，然后使用 `torch.tensor` 创建张量，并将这些张量按单词顺序堆叠，形成一个编码序列 `test_encoded`。\n",
    "   - 将每个编码序列的长度（`test_lens`）记录下来，用于后续处理。\n",
    "   - 将编码序列和对应的长度信息分别存储在 `test_encoded_list` 和 `test_encoded_lens` 列表中。\n",
    "- **构建 key-one-hot 编码映射字典**：\n",
    "   - 将图像的键与对应的编码序列以及长度信息一一映射，得到 `test_key_dict` 和 `test_key_dict_lens` 字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_tvt(dataname='train',word_to_idx=[]):\n",
    "    data=None\n",
    "    if dataname == 'train':\n",
    "        with open('data/deepfashion-multimodal/train.json','r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "    elif dataname == 'valid':\n",
    "        with open('data/deepfashion-multimodal/valid.json','r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "    elif dataname == 'test':\n",
    "        with open('data/deepfashion-multimodal/test_captions.json','r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "    descriptions = list(data.values()) # 保留',.'，去除其他标点符号的转换器\n",
    "    all_words = []\n",
    "    for description in descriptions:\n",
    "        # 先去除标点符号，然后进行分词\n",
    "        words = re.findall(r'\\b\\w+\\b|[,.‘]', description.lower())\n",
    "        all_words.extend(words)\n",
    "\n",
    "    encoded_list = []\n",
    "    encoded_lens = []\n",
    "    max_length=0\n",
    "    for description in descriptions:\n",
    "        words = description.lower().split()\n",
    "        words=['<sta>']+words\n",
    "        words.append('<eos>')\n",
    "        encoded = torch.stack([torch.tensor(word_to_idx.get(word, word_to_idx['<unk>'])) for word in words], dim=0)\n",
    "        lens = encoded.size()\n",
    "        if encoded.size(0)>max_length:\n",
    "            max_length=encoded.size(0)\n",
    "        encoded_list.append(encoded)\n",
    "        encoded_lens.append(lens)\n",
    "    # 6. 构建key-one-hot编码的映射字典\n",
    "    val_key_dict = dict(zip(data.keys(), encoded_list))\n",
    "    val_key_dict_lens = dict(zip(data.keys(), encoded_lens))\n",
    "\n",
    "    return val_key_dict , val_key_dict_lens , max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 获取训练数据中所有词汇及词汇表\n",
    "\n",
    "**1. 读取训练数据：**使用json库从一个JSON文件中读取训练数据集，这个数据集包含了图像和相关的文本描述。\n",
    "\n",
    "**2. 提取文本描述：**从读取的traindata中获取所有的文本描述，假设每个条目的值是一个文本描述。\n",
    "\n",
    "**3. 分词和去除标点：**通过正则表达式re.findall(r'\\b\\w+\\b|[,.!\"-]', description.lower())对每个文本描述进行分词，其中：\n",
    "    - \\b\\w+\\b 匹配字词边界之间的单词。\n",
    "    - [,.!\"-] 匹配特定的标点符号。\n",
    "    - description.lower() 将文本转换为小写，以保证大小写不会导致相同单词被视为不同的单词。\n",
    "\n",
    "**4. 构建词汇表：**使用Counter来统计所有单词的频率，并按频率从高到低排序。词汇表vocab被初始化为包含特殊标记<pad>（填充）和<sta>（开始），然后加上根据频率排序的所有单词，最后添加<unk>（未知）和<eos>（结束）标记。\n",
    "    \n",
    "**5. 创建单词到索引的映射：**word_to_idx是一个字典，它将每个单词映射到一个唯一的索引，这个索引是根据单词在vocab中的位置来确定的。\n",
    "\n",
    "**6. 创建索引到单词的映射：**idx_to_word是word_to_idx的反向映射，它将每个索引映射回原来的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入训练集\n",
    "with open('data/deepfashion-multimodal/train.json','r') as json_file:\n",
    "    traindata = json.load(json_file)\n",
    "# 获取所有文本描述\n",
    "descriptions = list(data.values())\n",
    "ALL_WORDS = []\n",
    "for description in descriptions:\n",
    "    # 先去除标点符号，然后进行分词\n",
    "    words = re.findall(r'\\b\\w+\\b|[,.!\"-]', description.lower())#.translate(TRANSALTOR))\n",
    "    ALL_WORDS.extend(words)\n",
    "\n",
    "# 4. 构建词汇表和映射\n",
    "word_counts = Counter(ALL_WORDS)\n",
    "vocab = ['<pad>','<sta>'] + [word for word, _ in word_counts.most_common()] +['<unk>','<eos>']\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 定义导入glove预训练数据集的函数\n",
    "\n",
    "- **`load_glove_embedding(glove_file)` 函数**：\n",
    "\n",
    "   - **输入参数**：GloVe 词嵌入文件的路径。\n",
    "\n",
    "   - **输出**：一个字典，其中键是词汇表中的单词，值是对应的词嵌入向量。\n",
    "\n",
    "   - **具体步骤**：\n",
    "     - 打开 GloVe 文件，逐行读取，每一行包含单词和其对应的词嵌入向量。\n",
    "     - 将单词作为字典的键，将对应的词嵌入向量（表示为 NumPy 数组）作为值。\n",
    "     - 返回包含所有单词和对应词嵌入向量的字典。\n",
    "  \n",
    "- **`create_embedding_matrix(vocab, embedding_dict, embedding_dim)` 函数**：\n",
    "\n",
    "   - **输入参数**：\n",
    "     - `vocab`：词汇表，包含模型使用的所有单词。\n",
    "     - `embedding_dict`：包含预训练词嵌入的字典。\n",
    "     - `embedding_dim`：词嵌入向量的维度。\n",
    "\n",
    "   - **输出**：一个二维 NumPy 数组，表示模型的词嵌入矩阵。\n",
    "\n",
    "   - **具体步骤**：\n",
    "     - 遍历词汇表中的每个单词。\n",
    "     - 如果单词在预训练的词嵌入字典中存在，则将对应的词嵌入向量赋给词嵌入矩阵的相应行。\n",
    "     - 如果单词不在预训练的词嵌入字典中，处理特殊标记 `<unk>`, `<pad>`, `<sta>`, `<eos>`，分别用随机向量、零向量或预训练的开始和结束标记向量填充。\n",
    "     - 返回创建的词嵌入矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embedding(glove_file):\n",
    "    embedding_dict = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_dict[word] = vector\n",
    "    return embedding_dict\n",
    "\n",
    "def create_embedding_matrix(vocab, embedding_dict, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[i] = embedding_dict[word]\n",
    "        else:\n",
    "            # 处理特殊标记，可以选择使用GloVe中的对应向量或随机初始化的向量\n",
    "            if word == '<unk>':\n",
    "                embedding_matrix[i] = np.random.rand(embedding_dim)\n",
    "            elif word == '<pad>':\n",
    "                embedding_matrix[i] = np.zeros(embedding_dim)\n",
    "            elif word == '<sta>' :\n",
    "                embedding_matrix[i] = embedding_dict['start']\n",
    "            elif word == '<eos>' :\n",
    "                embedding_matrix[i] = embedding_dict['end']\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.6 引入glove预训练的词嵌入模型\n",
    "-   首先，我们需要下载glove的预训练词嵌入模型glove.6B.50(100/200/300)d.txt，并将其解压到data/。，也可以选择50d 100d 200d，相应的EMBED_DIM参数也要修改。\n",
    "-   我们需要将glove的预训练词嵌入模型转换为PaddlePaddle可用的词嵌入模型。\n",
    "-   我们将转换后的词嵌入模型加载到PaddlePaddle中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'data/glove.6B.300d.txt'  # GloVe文件路径\n",
    "EMBED_DIM= 300  # 你选择的嵌入维度\n",
    "if os.path.exists(\"data/glove_vocab.pth\"):\n",
    "    print(\"glove 词典已划分!\")\n",
    "    embedding_matrix_tensor = torch.load(\"data/glove_vocab.pth\")\n",
    "    print(\"embedding tensor\",embedding_matrix_tensor)\n",
    "else:\n",
    "    print(\"glove 词典未划分，正在划分...\")\n",
    "    glove_vocab = \"data/glove_vocab.pth\"\n",
    "    # 从GloVe文件中加载嵌入向量\n",
    "    glove_embeddings = load_glove_embedding(glove_file)\n",
    "    # 创建词汇表-embedding矩阵字典\n",
    "    embedding_matrix = create_embedding_matrix(vocab, glove_embeddings, EMBED_DIM)\n",
    "    # 将 embedding_matrix 转换为 PyTorch 张量\n",
    "    embedding_matrix_tensor = torch.tensor(embedding_matrix)\n",
    "    torch.save(embedding_matrix_tensor, glove_vocab)\n",
    "    print(\"embedding matrix\",embedding_matrix)\n",
    "    print(\"embedding matrix shape\",embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.7 自定义数据集类，用于训练时批量读取数据\n",
    "- 重写__getitem__方法，返回一个batch的数据\n",
    "  - 这里既返回了使用glove预训练词嵌入模型处理后的句子embedded_captions，也返回了原始句子caption便于后续计算损失\n",
    "  - len返回了一个样本中句子的长度\n",
    "  - 为了便于批量读取成矩阵，故在这里面将所有句子填充到固定长度max_lengths，后续会切分\n",
    "- 重写__len__方法，返回数据集的总长度\n",
    "- 新增一个randomcheck方法，用于在模型自由生成时随机取数据集中一个样本输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, key_onehot_dict, key_onehot_dict_lens, max_length, transform=None):\n",
    "        self.key_onehot_dict = key_onehot_dict\n",
    "        self.key_onehot_dict_lens = key_onehot_dict_lens\n",
    "        self.max_length = max_length\n",
    "        self.keys = list(key_onehot_dict.keys())\n",
    "        # PyTorch图像预处理流程\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        image = Image.open(\"data/deepfashion-multimodal/images/\"+key).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        encoded = self.key_onehot_dict[key]\n",
    "        len = self.key_onehot_dict_lens[key]\n",
    "        # 填充到最大长度\n",
    "        pad_length = self.max_length - len[0]\n",
    "        # onehot_encoded_max = torch.stack([one_hot(torch.tensor(word_to_idx['<pad>']), len(vocab)) for i in range(pad_length)])\n",
    "        if pad_length > 0:\n",
    "            onehot_encoded_max = torch.stack([torch.tensor(word_to_idx['<pad>']) for i in range(pad_length)])\n",
    "            caption = torch.cat([encoded, onehot_encoded_max])\n",
    "        else:\n",
    "            # Handle the case where pad_length is 0\n",
    "            caption = encoded\n",
    "        # # print(onehot_encoded_max.size())\n",
    "        \n",
    "        # 使用 embedding_matrix 对张量中的每个元素进行查找\n",
    "        embedded_captions = F.embedding(caption.long(), embedding_matrix_tensor)\n",
    "\n",
    "        # print(caption.size())\n",
    "        return image, embedded_captions, len,caption\n",
    "    \n",
    "    def randomcheck(self):\n",
    "        random_image = random.choice(self.keys)\n",
    "        image = Image.open(\"data/deepfashion-multimodal/images/\"+random_image).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            inputimage = self.transform(image)\n",
    "                \n",
    "        onehot_encoded = self.key_onehot_dict[random_image]\n",
    "        inputcaption = torch.tensor(onehot_encoded, dtype=torch.float32).clone().detach().requires_grad_(True)\n",
    "        embedded_captions = F.embedding(inputcaption.long(), embedding_matrix_tensor)\n",
    "\n",
    "        # print(caption.size())\n",
    "        return image,inputimage,embedded_captions,inputcaption\n",
    "    \n",
    "def one_hot_to_index(one_hot):\n",
    "    return torch.argmax(one_hot, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.8 定义模型结构\n",
    "- EncoderCNN 类：\n",
    "  - 使用预训练的ResNet-50模型进行图像特征提取。\n",
    "  - 对提取的图像特征进行处理，通过一个全连接层 self.fc 调整输出的维度，最终输出形状为 (batch_size, pic_counts, embed_size)，pic_counts 表示每个图像提取的特征数量。\n",
    "- TransformerModel 类：\n",
    "  - 使用上述的 EncoderCNN 作为图像特征提取层，同时包含了Transformer模型的编码器和解码器。在编码器和解码器之间使用位置编码器 PositionalEncoding 来加入序列的位置信息。\n",
    "  - 最终的输出通过全连接层 self.fc 得到，并在最后一维上使用 softmax 归一化。\n",
    "  - 这两个predict函数是用于在训练好的图像描述生成模型上进行文本生成的方法。\n",
    "1. **predict方法**：\n",
    "   - 使用贪心算法生成文本序列。\n",
    "   - 输入一张图像，通过模型的图像编码器（`EncoderCNN`）提取图像特征，然后通过位置编码器处理，得到编码后的图像特征。\n",
    "   - 初始化一个包含起始符号 `<sta>` 的序列，然后通过循环生成文本序列。\n",
    "   - 在每个时间步，将当前序列传递到Transformer解码器（`TransformerDecoder`）中，获取最后一个时间步的输出。\n",
    "   - 使用全连接层进行预测，选择输出中概率最大的标记作为下一个时间步的预测。\n",
    "   - 将预测的标记添加到序列中，并检查是否生成了结束符 `<eos>`，如果是，则停止循环。\n",
    "\n",
    "2. **predict_beam_search方法**：\n",
    "   - 使用束搜索算法生成文本序列。\n",
    "   - 与贪心算法不同，此方法在每个时间步选择多个备选序列，并根据概率进行排序。\n",
    "   - 在每个时间步，根据当前备选序列，使用模型生成多个备选的下一个标记，并计算对应的分数（概率）。\n",
    "   - 将备选序列和分数按照分数进行排序，保留分数最高的一部分备选序列。\n",
    "   - 重复上述步骤，直到生成的序列达到最大长度或者包含结束符 `<eos>`。\n",
    "   - 返回得分最高的序列（去除开始符和结束符），作为最终生成的文本序列。\n",
    "\n",
    "这两种方法在生成文本时采用了不同的策略，贪心算法简单直观，而束搜索算法可以更全面地考虑多个备选序列，有助于生成更合理的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size,counts=50):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.counts = counts\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size * counts)  # Adjust the output size\n",
    "\n",
    "    def forward(self, images):\n",
    "        # (batch_size, channels, height, width)\n",
    "        features = self.resnet(images)\n",
    "        features = F.leaky_relu(features,0.2) \n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.fc(features)\n",
    "        features = features.view(features.size(0),self.counts, -1)  # Reshape to (batch_size, counts, embed_size)\n",
    "        return features\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    # 初始化权重\n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    '''\n",
    "    生成一个方形的mask，为下三角矩阵，长和宽为输入数据的序列长度\n",
    "    [[0.0, -inf, -inf],\n",
    "     [0.0,  0.0, -inf],\n",
    "     [0.0,  0.0,  0.0]]\n",
    "    '''\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers,pic_counts=10):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.cnn = EncoderCNN(d_model,pic_counts)                # 图像特征提取层\n",
    "        self.positional_encoding = PositionalEncoding(d_model)   # 位置编码器\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(        # transformer编码器\n",
    "            nn.TransformerEncoderLayer(d_model, nhead ),\n",
    "            num_encoder_layers\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(         # transformer解码器\n",
    "            nn.TransformerDecoderLayer(d_model, nhead ),\n",
    "            num_decoder_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.init_weights()  \n",
    "    def forward(self,imageinput, tgt):\n",
    "        '''\n",
    "        imageinput: 图像\n",
    "        tgt: 文本描述\n",
    "        输入形状   imageinput (batch_size, channels, height, width)\n",
    "                    tgt (batch_size, sequence, embedding_size)\n",
    "        '''\n",
    "        # ------------------------------------------编码器------------------------------------------\n",
    "        imageout = self.cnn(imageinput)\n",
    "        # print(\"imageout\",imageout.size())\n",
    "        imageout = imageout.permute(1, 0, 2)  # 将数据张量形状转化为 (sequence, batch_size, vocabsize)\n",
    "        src = self.positional_encoding(imageout)\n",
    "        # print(\"src\",src.size())\n",
    "\n",
    "        # ------------------------------------------解码器------------------------------------------\n",
    "        # 目标序列，文本描述\n",
    "        tgt = tgt.permute(1, 0, 2)  # 将数据张量形状转化为 (sequence, batch_size, data)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        # 序列掩码mask，不让模型看到待传入序列的后面的序列  为下三角矩阵\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        # print(\"tgtmask\",tgt_mask)\n",
    "        encode_output = self.transformer_encoder(src)\n",
    "        # print(\"encode_output\",encode_output.size())\n",
    "        decode_output = self.transformer_decoder(tgt,encode_output,tgt_mask=tgt_mask)\n",
    "        output = self.fc(decode_output)\n",
    "        # output = self.soft(output)\n",
    "        '''\n",
    "        输出形状   (batch_size, sequence, vocab_size)\n",
    "        '''\n",
    "        return output\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self,imageinput,max_length=50):    # 贪心算法生成文本\n",
    "        '''\n",
    "        在模型自由生成时采用贪心算法，每次预测取一个概率最大的词为预测结果\n",
    "        imageinput: 图像\n",
    "        输入形状   imageinput (batch_size, channels, height, width)\n",
    "        '''\n",
    "        # ------------------------------------图像编码-----------------------------------------\n",
    "        imageout = self.cnn(imageinput)\n",
    "        imageout = imageout.permute(1, 0, 2)\n",
    "        src = self.positional_encoding(imageout)  # sequence  batchsize  data   25 1 512\n",
    "        encode_output = self.transformer_encoder(src)\n",
    "\n",
    "        # ------------------------------------循环解码------------------------------------------\n",
    "        input_sequence = torch.tensor([[word_to_idx['<sta>']]]).to(device)\n",
    "        tgt = input_sequence\n",
    "        # 循环生成序列\n",
    "        for i in range(max_length):\n",
    "            # 位置编码\n",
    "            tgt = self.positional_encoding(tgt)\n",
    "\n",
    "            # 传递到Transformer模型\n",
    "            decode_output=self.transformer_decoder(tgt,encode_output)\n",
    "\n",
    "            # 获取最后一个时间步的输出\n",
    "            output_last_step = decode_output[-1, :, :]\n",
    "\n",
    "            # 使用全连接层进行预测\n",
    "            predicted_token = self.fc(output_last_step)\n",
    "        \n",
    "            # 获取预测的下一个token\n",
    "            predicted_token = torch.argmax(predicted_token, dim=-1)\n",
    "\n",
    "            if predicted_token.item() == word_to_idx['<eos>']:\n",
    "                break\n",
    "\n",
    "            # 将预测的token添加到输入序列中\n",
    "            input_sequence = torch.cat((input_sequence, predicted_token.unsqueeze(0)), dim=0)\n",
    "            # 将预测的token经过 glove 词嵌入后拼接到tgt后面\n",
    "            s = input_sequence.long().to('cpu')\n",
    "            general_squence = F.embedding(s, embedding_matrix_tensor).float()\n",
    "            tgt = general_squence.to(device)\n",
    "            \n",
    "\n",
    "        # 返回生成的文本序列（去除开始符和结束符）\n",
    "        generated_sequence = input_sequence[1:].squeeze().tolist()\n",
    "        '''\n",
    "        输出形状   list  例如['I','Love','You','.']\n",
    "        '''\n",
    "        return generated_sequence\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_beam_search(self, imageinput, beam_width=5, max_length=100):\n",
    "        '''\n",
    "        在模型自由生成时采用束搜索算法，每次预测取5个概率最大的词为候选词汇，下一次生成依次取这5个候选词汇生成新的词汇（25个）\n",
    "        从中再挑选5个概率最大的词汇作为下一次预测的输入\n",
    "\n",
    "        imageinput: 图像\n",
    "        beam_width: 束宽度\n",
    "        max_length: 生成序列最大长度\n",
    "\n",
    "        输入形状   imageinput (batch_size, channels, height, width)\n",
    "        '''\n",
    "        # ------------------------------------图像编码-----------------------------------------\n",
    "        imageout = self.cnn(imageinput)\n",
    "        imageout = imageout.permute(1, 0, 2)\n",
    "        src = self.positional_encoding(imageout)\n",
    "        encode_output = self.transformer_encoder(src)\n",
    "\n",
    "        # ------------------------------------循环解码-----------------------------------------\n",
    "        start_token = torch.tensor([[word_to_idx['<sta>']]]).to(device)\n",
    "        beam = [(start_token, 0)]\n",
    "        for i in range(max_length):\n",
    "            candidates = []\n",
    "            for seq, score in beam:\n",
    "                if seq[0].item() == word_to_idx['<eos>']:\n",
    "                    # 若生成序列已经结束，则直接将序列添加到候选序列中\n",
    "                    candidates.append((seq, score))\n",
    "                else:\n",
    "                    # 若生成序列还未结束，则将序列经过 glove 词嵌入后拼接到tgt后面\n",
    "                    seq = seq.long().to('cpu')\n",
    "                    general_seq = F.embedding(seq, embedding_matrix_tensor).float()\n",
    "                    seq = seq.to(device)\n",
    "                    general_seq = general_seq.to(device)\n",
    "                    # 传递到Transformer模型\n",
    "                    decode_output = self.transformer_decoder(general_seq, encode_output)\n",
    "                    output_last_step = decode_output[-1, :, :]\n",
    "                    predicted_token_probs = F.softmax(self.fc(output_last_step), dim=-1)\n",
    "                    # 取topk个概率最大的词汇\n",
    "                    top_k_probs, top_k_indices = torch.topk(predicted_token_probs, beam_width, dim=-1)\n",
    "                    # 遍历生成topk个概率最大的词汇\n",
    "                    for k in range(beam_width):\n",
    "                        next_token = top_k_indices[:, k].unsqueeze(0)\n",
    "                        next_score = score + torch.log(top_k_probs[:, k])\n",
    "                        next_seq = torch.cat((seq, next_token), dim=0)\n",
    "                        # 检查是否生成了<unk>，如果是则跳过\n",
    "                        if next_token.item() != word_to_idx['<unk>']:\n",
    "                            candidates.append((next_seq, next_score))\n",
    "\n",
    "            # 排序\n",
    "            beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            # 检查是否生成了终止符\n",
    "            if any(seq[0].item() == word_to_idx['<eos>'] for seq, _ in beam):\n",
    "                break\n",
    "\n",
    "        # 选择最终的序列\n",
    "        final_seq, _ = max(beam, key=lambda x: x[1])\n",
    "        # 返回生成的文本序列（去除开始符和结束符）\n",
    "        generated_sequence = final_seq[1:].squeeze().tolist()\n",
    "        return generated_sequence\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # 位置编码  采用传统公式\n",
    "    def __init__(self, d_model, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.9 定义模型及优化器\n",
    "\n",
    "1. 配置参数:\n",
    "    - MAX_LENGTHS, BATCH_SIZE, EPOCHS 设置数据处理和训练的基本参数。\n",
    "    - vocab_size 计算词汇表的大小，可能用于模型的输入处理。\n",
    "    - d_model, nhead, num_encoder_layers, num_decoder_layers 确定Transformer模型的关键参数，如嵌入维度、注意力头数、编码器和解码器的层数。\n",
    "    - learning_rate_cnn 和 learning_rate_transformer 设置CNN和Transformer的学习率。\n",
    "    \n",
    "2. 数据增强:\n",
    "    - transform2ResNet18 使用图像转换来增强数据，包括调整图像大小至224x224，转换为张量，并进行标准化。\n",
    "    \n",
    "3. 模型构建:\n",
    "    - 创建一个Transformer模型实例，它结合了CNN和Transformer结构。\n",
    "    \n",
    "4. 优化器和损失函数:\n",
    "    - 配置不同的学习率参数，特别是针对CNN和Transformer的不同部分。\n",
    "    - 使用交叉熵损失函数 (nn.CrossEntropyLoss) 和Adam优化器。\n",
    "    \n",
    "5. 数据加载:\n",
    "    - 使用DataLoader来批量加载训练、验证和测试数据集，允许数据的批量处理和洗牌。\n",
    "    \n",
    "6. 数据集实例创建:\n",
    "    - 创建训练、验证和测试数据集的实例，这些数据集应用了前面定义的数据增强。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTHS=120\n",
    "BATCH_SIZE=20\n",
    "EPOCHS=20\n",
    "# 数据增强\n",
    "transform2ResNet18 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "vocab_size = len(vocab)  # 词汇表大小\n",
    "d_model = EMBED_DIM  # 模型维度   \n",
    "nhead = 10 # 多头注意力的头数\n",
    "num_encoder_layers = 4 # 编码器层数\n",
    "num_decoder_layers = 4 # 解码器层数\n",
    "learning_rate_cnn = 1e-5     # resnet学习率\n",
    "learning_rate_transformer = 8e-5 # transformer学习率\n",
    "\n",
    "# 构建模型实例\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers,pic_counts=20).to(device)\n",
    "# 加载模型参数\n",
    "\n",
    "# 区分学习率参数\n",
    "parameters = [\n",
    "    {'params': model.cnn.parameters(), 'lr': learning_rate_cnn},\n",
    "    {'params': model.transformer_encoder.parameters(), 'lr': learning_rate_transformer},\n",
    "    {'params': model.transformer_decoder.parameters(), 'lr': learning_rate_transformer},\n",
    "]\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(parameters, lr= 8e-5)\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = CustomDataset(train_key_dict, train_key_dict_lens, max_length=MAX_LENGTHS, transform=transform2ResNet18)\n",
    "valid_dataset = CustomDataset(valid_key_dict, valid_key_dict_lens, max_length=MAX_LENGTHS, transform=transform2ResNet18)\n",
    "test_dataset = CustomDataset(test_key_dict, test_key_dict_lens, max_length=MAX_LENGTHS, transform=transform2ResNet18)\n",
    "\n",
    "# 创建 DataLoader 实例\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.10 模型训练\n",
    "\n",
    "1. 初始化日志记录器:\n",
    "    - 使用SummaryWriter来记录训练和验证过程中的损失，便于后续分析。\n",
    "    \n",
    "2. 训练周期:\n",
    "    - 使用for循环遍历每一个训练周期（epoch）。\n",
    "    \n",
    "3. 模型设置为训练模式:\n",
    "    - 调用model.train()，确保模型在训练模式下，启用诸如dropout等特性。\n",
    "    \n",
    "4. 训练过程:\n",
    "    - 遍历训练数据加载器（train_loader）中的每个批次（batch）。\n",
    "    - 对于每个batch，提取图像、字幕、长度和原始字幕。\n",
    "    - 根据最长的caption长度截断输入。\n",
    "    - 调整输入字幕，保留除终止符外的所有字符。\n",
    "    - 对模型进行前向传播，计算输出。\n",
    "    - 应用梯度裁剪以避免梯度爆炸。\n",
    "    - 将目标序列转换为one-hot编码形式，与模型输出调整至相同的维度。\n",
    "    - 计算损失函数。\n",
    "    - 执行反向传播（梯度下降）。\n",
    "    - 更新模型参数。\n",
    "    \n",
    "5. 验证过程:\n",
    "    - 将模型设置为评估模式（model.eval()），关闭训练时特有的特性如dropout。\n",
    "    - 禁用梯度计算，以节省计算资源和内存。\n",
    "    - 重复与训练类似的步骤，但不进行梯度下降和参数更新。\n",
    "    \n",
    "6. 损失计算和记录:\n",
    "    - 计算每个epoch的平均训练和验证损失。\n",
    "    - 使用writer.add_scalar记录训练和验证损失。\n",
    "    \n",
    "7. 模型保存:\n",
    "    - 在训练结束后，将模型的状态保存到文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# 模型训练代码，利用前文构造的dataloader训练模型\n",
    "writer = SummaryWriter(\"output\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Train_Epoch {epoch + 1}/{EPOCHS}'):\n",
    "        '''\n",
    "        从dataloader中取出batch数据，batch数据为一个tuple，包含以下内容：\n",
    "\n",
    "        images: (batchsize, channel, height, width)      数据图像\n",
    "        captions: (batchsize, max_caption_length)        经过glove词嵌入的文本，用于传入模型训练\n",
    "        lengths: (batchsize, 1)                          每个batch的文本序列长度，用于截断\n",
    "        origincaption: (batchsize, max_caption_length)   原始的文本序列，用于计算损失\n",
    "\n",
    "        '''\n",
    "        images = batch[0]\n",
    "        lengths = batch[2]\n",
    "        alo,_ = torch.max(lengths[0], dim=0)\n",
    "        # alo 记录了每一个batch中最长的caption长度，将其作为输入的长度\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # 先取进行了glove词嵌入摸型的文本序列\n",
    "        origincaption = batch[3]\n",
    "        # 因为我们使用了padding，所以实际上输入的caption长度可能比我们所定义的长度要长，将其截断\n",
    "        origincaption = origincaption[:, :int(alo.item())]\n",
    "        oucaptions = origincaption[:, 1:]  # 期望输出的目标序列，不带起始符\n",
    "        # print(\"oucaptions\",oucaptions.size())\n",
    "        # print(\"captions\",captions.size())\n",
    "        \n",
    "        # captions为字符索引形式，这里存放的都是索引值  captions (batchsize, max_caption_length)  \n",
    "        captions = batch[1]  \n",
    "        captions = captions[:, :int(alo.item()),:]\n",
    "        incaptions = captions[:, :-1,:]                 # 输入模型训练的caption，不包含终止符\n",
    "        # print(\"incaptions\",incaptions.size())\n",
    "       \n",
    "        \n",
    "        captions = captions.to(device).float()\n",
    "        incaptions = incaptions.to(device).float()\n",
    "        oucaptions = oucaptions.to(device).long()\n",
    "        \n",
    "        images=images.to(device)\n",
    "        out = model(images,incaptions)\n",
    "        # print(\"out\",out.size())                             # 注意：输出后batchsize维度在第二位了，第一位是序列维度\n",
    "        #                                                      out  ( max_caption_length, batchsize, vocab_size)\n",
    "        # print(\"captions\",captions.size())                   # captions (batchsize, max_caption_length)\n",
    "\n",
    "        out_adjusted = out.transpose(1, 0).float().to(device) # 调整维度，将batchsize维度放到第一位，max_caption_length维度放到第二位\n",
    "\n",
    "\n",
    "        # print(\"out_adjusted\",out_adjusted.size())\n",
    "        # print(\"captions_one_hot\",captions_one_hot.size())\n",
    "    \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)   # 防止梯度爆炸\n",
    "        # 将字符索引的目标序列转换为one-hot形式，并将其和输出一起调整维度，计算损失\n",
    "        # captions_one_hot  (batchsize * sequence_lengths , vocab_size)\n",
    "        # out_adjusted      (batchsize * sequence_lengths , vocab_size)\n",
    "        captions_one_hot = F.one_hot(oucaptions, num_classes=len(vocab)).float()  \n",
    "        captions_one_hot = captions_one_hot.reshape(-1, captions_one_hot.shape[-1])\n",
    "        out_adjusted = out_adjusted.reshape(-1, out_adjusted.shape[-1])   \n",
    "        \n",
    "        # print(\"incaptions\",incaptions.size())\n",
    "        # print(\"oucaptions\",oucaptions.size())\n",
    "        # print(\"captions_one_hot\",captions_one_hot.size())\n",
    "        # print(\"out_adjusted\",out_adjusted.size())\n",
    "\n",
    "        loss = criterion(out_adjusted, captions_one_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 接下来遍历验证集，查看模型在验证集上的性能，代码同上\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=f'Valid_Epoch {epoch + 1}/{EPOCHS}'):\n",
    "            images = batch[0]\n",
    "            lengths = batch[2]\n",
    "            alo,_ = torch.max(lengths[0], dim=0)\n",
    "            origincaption = batch[3]\n",
    "            origincaption = origincaption[:, :int(alo.item())]\n",
    "            oucaptions = origincaption[:, 1:] \n",
    "            captions = batch[1] \n",
    "            captions = captions[:, :int(alo.item()),:]\n",
    "            incaptions = captions[:, :-1,:] \n",
    "            captions = captions.to(device).float()\n",
    "            incaptions = incaptions.to(device).float()\n",
    "            oucaptions = oucaptions.to(device).long()\n",
    "            images=images.to(device)\n",
    "            out = model(images,incaptions)\n",
    "            out_adjusted = out.transpose(1, 0).float().to(device)\n",
    "            captions_one_hot = F.one_hot(oucaptions, num_classes=len(vocab)).float()  \n",
    "            captions_one_hot = captions_one_hot.reshape(-1, captions_one_hot.shape[-1])\n",
    "            out_adjusted = out_adjusted.reshape(-1, out_adjusted.shape[-1]) \n",
    "            loss = criterion(out_adjusted, captions_one_hot)\n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / (len(train_loader)+1e-10)\n",
    "    valid_average_loss = valid_loss / (len(valid_loader)+1e-10)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{EPOCHS}], Train-Loss: {average_loss:.10f}, Valid-Loss: {valid_average_loss:.10f}')\n",
    "    writer.add_scalar(f\"nhead{nhead}_en{num_encoder_layers}_de{num_decoder_layers}_epoch{EPOCHS}/train_Loss\", average_loss, epoch)\n",
    "    writer.add_scalar(f\"nhead{nhead}_en{num_encoder_layers}_de{num_decoder_layers}_epoch{EPOCHS}/valid_Loss\", valid_average_loss, epoch)\n",
    "    \n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), f'./model/RT_model_{nhead}_{num_encoder_layers}+{num_decoder_layers}epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.11 定义三种评估方法的计算\n",
    "- BLEU   \n",
    "  - **compute_bleu_score( beam_widths=5, max_lengths=100)**\n",
    "  - BLEU 是一种用于自动评估机器翻译结果的指标。其公式如下：\n",
    "  - $$\\text{BLEU} = \\text{BP} \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\cdot \\log \\left(\\frac{\\text{Clip}_n}{\\text{Total}_n}\\right)\\right) $$\n",
    "  - 其中：\n",
    "    - $ \\text{BP} $ 是短句子惩罚（Brevity Penalty），用于惩罚生成的翻译比参考翻译短的情况。\n",
    "    - $ N $ 是 n-gram 的最大阶数。\n",
    "    - $ w_n $ 是每个 n-gram 的权重，通常设置为 $ \\frac{1}{N} $。\n",
    "    - $ \\text{Clip}_n $ 是每个 n-gram 类别下的预测 n-gram 数量与参考翻译中的 n-gram 数量的较小值。\n",
    "    - $ \\text{Total}_n $ 是预测翻译中的 n-gram 总数。\n",
    "\n",
    "- METEOR \n",
    "  - **compute_meteor_score(beam_widths=5, max_lengths=100)**\n",
    "  - METEOR的计算公式涉及多个步骤，包括计算精确匹配、计算词干和形态学变化、计算重叠、以及计算最终的 METEOR 得分。METEOR 的计算过程如下：\n",
    "    - **精确匹配**：计算生成的翻译与参考翻译在词汇上的精确匹配。\n",
    "  \n",
    "    - **词干和形态学变化**：对于不同形态的词汇，进行处理以考虑它们的相似性。\n",
    "\n",
    "    - **重叠**：对于重叠的词汇，进行惩罚。\n",
    "\n",
    "    - **最终 METEOR 得分计算**：综合以上步骤得到最终的 METEOR 得分。\n",
    "\n",
    "- ROUGE-L  \n",
    "  - **compute_rouge_l_score(beam_width=5, max_length=100)**\n",
    "  - ROUGE-L 是一种用于评估文本摘要质量的指标，特别是在生成的摘要和参考摘要之间的重叠度。其公式为：\n",
    "  - $$\\text{ROUGE-L} = \\frac{\\text{Longest Common Subsequence (LCS)}}{\\text{Total Reference Words}} $$\n",
    "  - 其中：\n",
    "    - $\\text{Longest Common Subsequence (LCS)}$ 表示最长的连续共同子序列的长度。\n",
    "    - $\\text{Total Reference Words}$ 表示参考摘要中词汇的总数。\n",
    "  - LCS 表示最长公共子序列，即最长的连续共同子序列的长度。ROUGE-L 考虑了生成摘要和参考摘要之间的相似度，越接近1表示生成摘要越接近参考摘要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLUE\n",
    "# BLEU值的计算\n",
    "def compute_bleu_score(beam_widths=5, max_lengths=100):\n",
    "    \n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    average_bleu = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=f'BLEU is calculating...'):\n",
    "            references = []\n",
    "            candidates = []\n",
    "            images = batch[0].to(device)\n",
    "            captions = batch[1].to(device)\n",
    "            for i in range(images.size(0)):\n",
    "                image = images[i].unsqueeze(0).to(device)\n",
    "                reference = captions[i].tolist()\n",
    "                # candidate = model.predict_beam_search(image)\n",
    "                candidate = model.predict_beam_search(image, beam_width=beam_widths, max_length=max_lengths)\n",
    "                references.append(reference)  # Remove the extra list around reference\n",
    "                candidates.append(candidate)\n",
    "            # references and candidates should be lists of strings\n",
    "            references = [[str(token) for token in reference] for reference in references]\n",
    "            candidates = [str(token) for token in candidates]\n",
    "            bleu_score = corpus_bleu(references, candidates, smoothing_function=smoothing_function)\n",
    "            average_bleu += bleu_score\n",
    "        average_bleu /= len(test_loader)\n",
    "    average_bleu = round(average_bleu, 4)\n",
    "    return average_bleu\n",
    "\n",
    "# rougel    \n",
    "# rouge的计算\n",
    "def compute_rouge_l_score(beam_width=5, max_length=100):\n",
    "    rouge_scorer = Rouge()\n",
    "    average_rouge = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=f'Rouge-L is calculating...'):\n",
    "            references = []\n",
    "            candidates = []\n",
    "            images = batch[0].to(device)\n",
    "            captions = batch[1].to(device)\n",
    "            for i in range(images.size(0)):\n",
    "                image = images[i].unsqueeze(0).to(device)\n",
    "                reference = ' '.join([str(token) for token in captions[i].tolist()])\n",
    "\n",
    "                candidate = model.predict(image)\n",
    "                # candidate = model.predict_beam_search(image, beam_width=beam_width, max_length=max_length)\n",
    "                candidate = ' '.join([str(token) for token in candidate])\n",
    "\n",
    "                references.append(reference)\n",
    "                candidates.append(candidate)\n",
    "            scores = rouge_scorer.get_scores(candidates, references, avg=True)\n",
    "            rouge_l_score = scores['rouge-l']['f']\n",
    "            average_rouge += rouge_l_score\n",
    "    average_rouge /= len(test_loader)  # 计算平均值\n",
    "    average_rouge = round(average_rouge, 4)\n",
    "    return average_rouge\n",
    "\n",
    "# Meteor 计算\n",
    "def compute_meteor_score(beam_widths=5, max_lengths=100):\n",
    "    average_meteor = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f'METEOR is calculating...'):   \n",
    "            references = []\n",
    "            candidates = []\n",
    "            images = batch[0].to(device)\n",
    "            captions = batch[1].to(device)\n",
    "            for i in range(images.size(0)):\n",
    "                image = images[i].unsqueeze(0).to(device)\n",
    "                reference = captions[i].tolist()\n",
    "                candidate = model.predict(image)\n",
    "                # candidate = model.predict_beam_search(image, beam_width=beam_widths, max_length=max_lengths)\n",
    "                references.append(' '.join(map(str, reference))) \n",
    "                candidates.append(' '.join(map(str, candidate)))  \n",
    "            meteor_score_value = meteor_score(references, candidates)\n",
    "            average_meteor += meteor_score_value\n",
    "    average_meteor /= len(test_loader)\n",
    "    average_meteor = round(average_meteor, 4)\n",
    "    return average_meteor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、实验结果及分析\n",
    "\n",
    "## 5.1 CNN+GRU\n",
    "\n",
    "### 5.1.1 训练结果（截取几个epoch）\n",
    "\n",
    "![title](cr-epoch1.png)\n",
    "\n",
    "- 上图是第1个epoch的训练结果，可以看到Loss值为2.9508，对于第一个epoch而言，这个损失值较高，意味着模型的预测与真实标签之间差异较大。从结果来看，模型输出了重复且不完整的文本，显示为“the person is wearing a sleeve sleeve with with ...”，还伴随着大量的“<pad>”填充符号，这表明模型目前还未能有效学习到生成准确描述的能力。\n",
    "\n",
    "![title](cr-epoch6.png)\n",
    "\n",
    "- 上图是第6个epoch的训练结果，可以看到Loss值为1.1081，模型的预测精度相比初期有所提高，但仍有改善空间。从结果来看，生成的文本“the tank shirt has sleeves sleeves, cotton fabric and pure color ... her wrist”虽然含有重复词汇和不完整的句子，但相比于之前的训练结果，可以看出模型在理解图像内容和生成相关描述方面有了一定的进步，比如提到了“cotton fabric”（棉质面料）和“pure color”（纯色）。同时，文本末尾的<eos>标记表示句子结束，这是模型学习到的结构化输出的一个正面信号。\n",
    "    \n",
    "![title](cr-epoch11.png)\n",
    "\n",
    "- 上图是第11个epoch的训练结果，可以看到Loss值为0.4689，显示出比之前周期更好的学习效果。文本描述中正确提到了衬衫的特征：“短袖”（neckline）、“棉质面料”（lower clothing）和“图案”（pants）。但是，描述中有些部分如“neckline”和“the pants”出现了不必要的重复，这表明模型在生成连贯句子方面还存在问题。尽管描述不完美，但模型能够使用结束符号<eos>来标记句子结束，这是正确的序列生成行为。\n",
    "    \n",
    "![title](cr-epoch20.png)\n",
    "\n",
    "- 上图是第20个epoch的训练结果，可以看到Loss值为0.2586，这通常意味着模型的性能有了显著提高。从结果来看，生成的文本仍然显示出一些问题，如“tank tank shirt”和“patterns patterns”中的重复词汇，以及没有准确描述图中服装的“连体裤”，而是错误地多次提到“the pants”。尽管如此，模型能够使用<eos>标记来结束句子，这表明它已经能够在一定程度上理解句子结构，仍有一定的改善空间。\n",
    "    \n",
    "### 5.1.2 评估方法结果（METEOR和ROUGE-L）\n",
    "\n",
    "![title](none.png)\n",
    "\n",
    "- 上图是参考的文本描述句子所对应的图片。\n",
    "\n",
    "![title](sen.png)\n",
    "\n",
    "- 上面是参考的文本描述句子（reference）和生成（预测）的文本描述句子（pred）。\n",
    "\n",
    "- 采用了METEOR和ROUGE-L这两种评估方法（标准）来对参考的文本描述句子（reference）和生成（预测）的文本描述句子（pred）进行评估。\n",
    "\n",
    "![title](sen1.png)\n",
    "\n",
    "- **1. METEOR评分: 0.4046**\n",
    "    - METEOR评分考虑了准确性、单词顺序和同义词的匹配。分数范围从0（不匹配）到1（完美匹配）。\n",
    "    - 分数为0.4046表明预测描述与参考描述有相当程度的语义对应，但仍然有不少的差距。\n",
    "\n",
    "- **2. ROUGE-L评分: 0.4615**\n",
    "    - ROUGE-L评分基于最长公共子序列，用来衡量摘要的质量。分数越高，表示生成的摘要与参考摘要的重叠度越高。\n",
    "    - 分数为0.4615意味着生成的描述在结构上与参考描述有一定的一致性，但仍然有不少的提升空间。\n",
    "    - 总之，模型在理解和生成服饰描述的任务上有一定的能力，但仍有改进空间。文本生成中的冗余和不连贯性对评分产生了负面影响，这些是未来模型改进时需要特别关注的问题。另外，模型需要更好地学习如何捕捉和描述图像中的细节，以便更精确地反映出参考描述中的信息。\n",
    "\n",
    "### 5.1.3 Loss Diagram Over Epoch\n",
    "\n",
    "![title](loss1.png)\n",
    "\n",
    "- 上图是模型的损失值随epoch变化的曲线图。从该图中我们可以看出：\n",
    "\n",
    "    - 1.损失下降：图中损失值从接近3.0开始，在第一个epoch后急剧下降，随后逐渐趋于平稳，并在20个epoch后降到约0.5。这表明模型在训练初期迅速学习，随着时间的推移损失值持续但以较慢的速度下降。\n",
    "\n",
    "    - 2.模型收敛：损失值的下降趋势表明模型正逐步收敛。在大约15个epoch之后，损失值下降速度变得非常缓慢，这表明模型已接近其性能的极限，或者需要更复杂的模型结构或调整训练策略来进一步减少损失值。\n",
    "\n",
    "    - 3.训练效果：初始损失值较高意味着模型开始时对数据的拟合不好，但随着训练的进行，模型能够更好地理解训练数据，损失值的下降表明模型预测的准确性在提高。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 网格/区域表示、ResNet+Transformer解码器\n",
    "\n",
    "### 5.2.1 训练结果（截取几个epoch）\n",
    "\n",
    "Epoch [21/30], Train-Loss: 0.3058105816, Valid-Loss: 0.3195542960\n",
    "\n",
    "![title](21.png)\n",
    "\n",
    "- 上图是第21个epoch的训练结果，训练损失train-loss和验证损失valid-loss分别为0.3058105816和0.3195542960。训练损失和验证损失相近，这通常表明模型没有出现严重的过拟合。损失值相对较低，表明模型在拟合训练数据方面表现良好，并能够在一定程度上泛化到验证集。文本中出现了短语“the lady is wearing a pair of”重复，这种通常是序列生成模型在某些权重过大导致的反馈循环，或者训练数据中存在的某些过度重复的模式导致模型学习到了错误的依赖。表明模型还有一定的提升空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch [24/30], Train-Loss: 0.2947880137, Valid-Loss: 0.3207431528\n",
    "        \n",
    "![title](24.png)\n",
    "\n",
    "- 上图是第24个epoch的训练结果，训练损失train-loss和验证损失valid-loss分别为0.2947880137和0.3207431528。这表明模型在训练集上的表现略好于验证集，但损失值相差不大，没有严重的过拟合。生成的文本描述显示了明显的问题：存在重复与不符，例如将图中的男性错误地称为“the lady”，并且描述中的“ring”和“patterns”与图像内容不相关。此外，文本缺乏结构上的连贯性和语法准确性。这些问题表明模型在文本生成方面还有相当的改进空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch [27/30], Train-Loss: 0.2790672440, Valid-Loss: 0.3300388249\n",
    "        \n",
    "![title](27.png)\n",
    "\n",
    "- 上图是第27个epoch的训练结果，训练损失train-loss和验证损失valid-loss分别为0.2790672440和0.3300388249。模型的训练损失进一步下降到0.2791，而验证损失略有上升至0.3300。这种情况通常表明模型可能开始出现过拟合的迹象，即它在训练数据上的表现比在未见过的数据上要好。文本描述出现了“solid color patterns”的重复，模型在理解图像内容和转换为相关描述方面还有一定的改善空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch [30/30], Train-Loss: 0.2532717405, Valid-Loss: 0.3560079117\n",
    "        \n",
    "![title](30.png)\n",
    "\n",
    "- 上图是第30个epoch的训练结果，训练损失train-loss和验证损失valid-loss分别为0.2532717405和0.3560079117。在第30个epoch结束时，模型的训练损失进一步下降到0.2533，但验证损失上升到0.3560。这个较大的差距可能表明模型在训练数据上过拟合，这意味着它可能记住了训练数据的特定特征，而不是学习到如何泛化到未见过的数据。文本描述中存在着“tank top tank top”和“solid color patterns”的重复，这表明模型在模型在泛化和语言生成方面仍然具有相当的优化空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Loss Diagram Over Epoch\n",
    "\n",
    "![title](loss2.png)\n",
    "\n",
    "上图是模型的训练损失（train_loss）和验证损失（valid_loss）随epoch变化的曲线图。从该图中我们可以看出：\n",
    "\n",
    "- **1.损失下降趋势**：两张图分别显示了训练损失（train_loss）和验证损失（valid_loss）随着epoch增加而下降的趋势，这表明模型在学习过程中持续改进，并且在拟合训练数据方面表现良好。\n",
    "\n",
    "- **2.训练损失和验证损失的关系**：在训练初期，损失急剧下降，然后逐渐平稳。这是一个典型的模型训练行为，其中模型在初期迅速捕捉数据的主要特征，随后改进速度放缓。训练损失（train_loss）的图显示，到了第20个epoch，损失值稳定在0.3025左右。而验证损失（valid_loss）也显示了类似的下降趋势，第20个epoch时稳定在0.3037左右。\n",
    "\n",
    "- **3.是否可能发生过拟合**：通常，如果验证损失随着时间的推移开始上升，则表示模型开始过拟合。但在这里，我们看不到验证损失有明显上升的迹象，这说明模型在给定的20个epoch内保持了一致的泛化能力。\n",
    "\n",
    "- **4.训练和验证损失的差距**：两者之间的差距相对较小，这通常表明模型在训练数据和未见过的数据上具有一致的表现。如果这个差距非常大，那可能是过拟合的标志。\n",
    "\n",
    "- **5.模型的稳定性**：由于训练损失和验证损失都趋于平稳，这表明模型可能接近收敛。在这种状态下，增加更多的训练周期可能不会带来显著的性能提升，而应考虑其他方法来改进模型性能，如调整模型结构或超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、总结\n",
    "\n",
    "- 在此次实验中，通过使用深度学习模型对服饰图像进行描述生成，我们得到了一系列有价值的经验与教训。\n",
    "\n",
    "- 首先，**模型结构的选择**对任务有显著的影响。**CNN+GRU模型**利用了**CNN强大的图像特征提取能力**和**GRU在序列建模**方面的优势。**ResNet+Transformer模型**则依赖于**ResNet的深度残差网络来捕捉深层图像特征**，并使用**Transformer结构**来**处理序列依赖关系**，这在理论上应对更复杂的图像描述任务更为有效。\n",
    "\n",
    "- 其次，**数据预处理**是确保模型正确学习的基础。文本的预处理包**括清洗、分词、构建词汇表、将文本转换为数值形式**等步骤，这些都是为了将自然语言转化为模型能够理解和处理的格式。图像的预处理同样重要，正确的**大小调整、归一化和数据增强**都能够帮助模型更好地从图像中提取特征。\n",
    "\n",
    "- 在模型训练过程中，我们通过**损失值**的下降来观察模型的学习进度，**损失值的显著降低表示模型在拟合训练数据上取得了进步**。然而，**损失值并不能完全代表模型的泛化能力**，因此在未来的工作中，应该**结合验证集和测试集上**的表现来全面评估模型性能。\n",
    "\n",
    "- 对于**模型评估**，**METEOR和ROUGE-L评估指标**提供了**文本生成质量的定量度量**。**METEOR**考虑了**翻译的准确性和流畅性**，而**ROUGE-L**关注的是**生成文本和参考文本之间的重叠程度**。评估结果表明，尽管模型能够生成与图片内容相关的描述，但在某些情况下还存在生成冗余或不精确描述的问题。**BLEU指标**的引入为评估提供了重要的补充，它通过**量化生成文本和参考文本之间的n-gram匹配程度来衡量翻译质量**。尽管模型能够产生与图像内容相关的描述，但BLEU分数揭示了在词汇的精确选择和语序方面模型仍有提升空间。\n",
    "\n",
    "- 通过这次实验，我们了解到深度学习模型在实际应用中的潜力和局限性。模型的训练和优化是一个**迭代**的过程，需要综合考虑**模型结构、数据处理、训练策略和评估方法**。此外，实验也揭示了在模型设计和优化中需要注意的关键点，如**防止过拟合、调整模型复杂度和超参数**、以及探索更高效的训练方法。总的来说，这次实验不仅提高了我们对深度学习在图像描述生成领域应用的认识，也为未来的研究方向提供了宝贵的经验和启示。\n",
    "\n",
    "- 同时，在本学期即将画上句号之际，我们对本门课程的袁彩霞老师和方全老师表示由衷地感谢！ 在本学期“神经网络与深度学习”和“神经网络与深度学习课程设计”两门课程中，袁老师和方老师进行了悉心的备课，准备了清晰明了的课件，课堂上循循善诱，引导着同学们在一个个模型与网络中感受深度学习之美。并且，无论是在课堂上还是课堂下，袁老师与方老师都十分耐心地回答了我们在神经网络与深度学习的学习过程中遇到的各种问题。同时，“神经网络与深度学习”课程的9个实验（全连接网络、卷积神经网络、图卷积网络、循环神经网络、特征学习、生成模型和强化学习等）和“神经网络与深度学习课程设计”的服饰图像描述生成的作业不仅帮助我们巩固了神经网络和深度学习的相关知识，将理论与实践结合起来，还增强了我们的学习能力和独立解决问题的能力。这将对我们今后的学习产生重要的影响。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
